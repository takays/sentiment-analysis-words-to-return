{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNxC4PlTXl2ghGB33uDu+8O"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### DeBERTa Finetuning\n",
        "\n"
      ],
      "metadata": {
        "id": "pQIgcy8Dlq7m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXMbAxN0BNVf"
      },
      "source": [
        "#### Google Drive mount & Set up Data Folder & Set up libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAfs6WJxA30u"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch] datasets scikit-learn pandas accelerate -U"
      ],
      "metadata": {
        "id": "gdDRmhz1P2md",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUの確認\n",
        "import torch\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "VYhylq0m-9dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### chABSA-dataset を使用"
      ],
      "metadata": {
        "id": "Iq6509bfBXWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset, load_metric\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid\n",
        "import numpy as np\n",
        "\n",
        "# CSVファイルのロード\n",
        "data = pd.read_csv('./chABSA_posneg.csv')\n",
        "\n",
        "# データをトレーニングセットとテストセットに分割\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    data['text'].tolist(), data['label'].tolist(), test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# データセットの作成\n",
        "train_data = pd.DataFrame({'text': train_texts, 'label': train_labels})\n",
        "test_data = pd.DataFrame({'text': test_texts, 'label': test_labels})\n",
        "\n",
        "# Hugging Face datasets形式に変換\n",
        "train_dataset = Dataset.from_pandas(train_data)\n",
        "test_dataset = Dataset.from_pandas(test_data)"
      ],
      "metadata": {
        "id": "Jktg6NlfFwR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "NAeWcxz1Ayvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 日本語の事前学習済みモデルとトークナイザーをロード\n",
        "model_name = \"izumi-lab/deberta-v2-base-japanese\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # ラベル数を2に設定\n",
        "\n",
        "# モデルをGPUに移動\n",
        "model.to(device)\n",
        "\n",
        "# データの前処理\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# データセットのフォーマット\n",
        "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
        "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SXFeFQhRA7Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 評価メトリックの定義\n",
        "metric = load_metric(\"f1\", trust_remote_code=True)\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=1)\n",
        "    f1 = metric.compute(predictions=preds, references=labels, average='macro')\n",
        "    return f1\n",
        "\n",
        "# パラメータグリッドの定義\n",
        "param_grid = {\n",
        "    'learning_rate': [3e-5, 2e-5, 5e-5],\n",
        "    'per_device_train_batch_size': [8, 16],\n",
        "    'num_train_epochs': [2, 3]\n",
        "}\n",
        "\n",
        "best_f1 = 0\n",
        "best_params = {}"
      ],
      "metadata": {
        "id": "kcl6GyIKBtZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# グリッドサーチの実行\n",
        "for params in ParameterGrid(param_grid):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./DeBERTa/results_f1\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        learning_rate=params['learning_rate'],\n",
        "        per_device_train_batch_size=params['per_device_train_batch_size'],\n",
        "        per_device_eval_batch_size=params['per_device_train_batch_size'],\n",
        "        num_train_epochs=params['num_train_epochs'],\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./DeBERTa/logs_f1',\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"epoch\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # トレーニングの実行\n",
        "    trainer.train()\n",
        "\n",
        "    # 評価の実行\n",
        "    results = trainer.evaluate()\n",
        "    print(f\"Params: {params}, Results: {results}\")\n",
        "\n",
        "    # ベストパラメータの更新\n",
        "    if results['eval_f1'] > best_f1:\n",
        "        best_f1 = results['eval_f1']\n",
        "        best_params = params\n",
        "\n",
        "# ベストパラメータで再トレーニング\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./DeBERTa/best_model_f1\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=best_params['learning_rate'],\n",
        "    per_device_train_batch_size=best_params['per_device_train_batch_size'],\n",
        "    per_device_eval_batch_size=best_params['per_device_train_batch_size'],\n",
        "    num_train_epochs=best_params['num_train_epochs'],\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./DeBERTa/logs_f1',\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# トレーニングの実行\n",
        "trainer.train()\n",
        "\n",
        "# モデルの保存\n",
        "model.save_pretrained(\"./DeBERTa/best_model_f1\")\n",
        "tokenizer.save_pretrained(\"./DeBERTa/best_model_f1\")\n",
        "\n",
        "print(f\"Best Params: {best_params}, Best F1: {best_f1}\")"
      ],
      "metadata": {
        "id": "c5nFK6KaCK_0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
