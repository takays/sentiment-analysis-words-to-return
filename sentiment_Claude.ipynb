{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQIgcy8Dlq7m"
      },
      "source": [
        "### Claude Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Y4dTxQMdUzdg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MGDU_fARUzdg"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Kenkyu/Finance/2024/data/'\n",
        "%cd $path\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gRnK7rJRdSlN"
      },
      "outputs": [],
      "source": [
        "!pip install anthropic\n",
        "import anthropic\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('claude_fin')\n",
        "\n",
        "client = anthropic.Anthropic(api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oyyc8H4fvUSr"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import re\n",
        "import pandas as pd\n",
        "from anthropic import Anthropic\n",
        "\n",
        "max_chars_per_chunk = 100000 - 500\n",
        "\n",
        "def analyze_sentiment(df, api_model):\n",
        "    instruction = f\"\"\"あなたは証券アナリストです。以下の企業の開示情報を読み、文脈も考慮して、positive_sentiment_scoreとnegative_sentiment_scoreを合計100になるように算出しなさい。\n",
        "    ルール)\n",
        "    フォーマットは以下で出力しなさい。positive_score: X(0〜100) negative_score: Y(0〜100)\n",
        "    positive_scoreとnegative_scoreは、両方合わせて100になるようにスコアを算出しなさい。\n",
        "    例を参考にして開示情報の内容全体を漏れなく厳密に評価しなさい。\n",
        "    理由は出力しないで、positive_score: X(0〜100) negative_score: Y(0〜100)のみを出力してください。\n",
        "    ・positive_scoreが100の開示情報の例\n",
        "    「セグメント利益は、販売数量の増加により、同247百万円増益の178百万円となりました」\n",
        "    「米国では企業業績や個人消費が堅調に推移し、景気は緩やかに回復しました」\n",
        "    「営業利益は、非鉄金属相場や為替相場の変動に伴うたな卸資産の在庫影響（以下「在庫要因」）が好転し、機能材料部門において主要製品の販売量が増加したこと等により、前連結会計年度に比べて273億円（245.3%）増加の384億円となりました」\n",
        "    「期間限定で東京駅一番街にオープンした当社初のアンテナショップ「パティスリーブルボン」では、特別に仕立てたクッキーの限定商品「ラングレイス」や「ルマンドアソート」などに大きな反響をいただきました」\n",
        "    「セグメント利益は、のれん償却費３億33百万円を計上したものの、上記要因に伴う営業利益の増加に加え、飲食用資材分野における原材料価格の下落などにより、９億45百万円と前年同期比２億41百万円（34.3%）の増益となりました」\n",
        "    ・negative_scoreが100の開示情報の例\n",
        "    「建築用塗料を取扱う塗料部門におきましては、新築向け市場及びリフォーム向け市場とも、工事を伴う施工棟数が前年度に比べ伸び悩んだことなどにより、売上高は減少いたしました」\n",
        "    「この結果、売上高は126億17百万円（同4.8％減）となり、営業利益は７億40百万円（同11.2％減）となりました」\n",
        "    「即席麺部門は、製造ラインの移設に伴う稼働率の低下と受託が低調に推移し、また、３月に製造ラインを増設しましたが、売上の寄与は低く、売上高は7,085百万円と前年同期と比べ659百万円（8.5％）の減収となり、セグメント利益（営業利益）は204百万円と前年同期と比べ219百万円（51.8％）の減益となりました」\n",
        "    「しかしながら、資材費や労務費のコストが高止まりする中で、北海道・東北地区の集中豪雨の影響により、公共工事の優先順位が入れ替わり、当初予定されていた工期が先延ばしになるなど、当社を取り巻く経営環境は厳しい状況で推移しました」\n",
        "    「当事業年度におけるわが国経済は、政府による経済政策や金融政策の総動員もあり、緩やかな回復基調となったものの、個人消費や設備投資は力強さを欠き、海外経済の減速と為替、原材料価格の変動リスクを抱え、先行き不透明な状況が続いた」\"\"\"\n",
        "\n",
        "    def split_text(text, max_chars=max_chars_per_chunk):\n",
        "        sentences = text.split('。')\n",
        "        chunks = []\n",
        "        current_chunk = \"\"\n",
        "        current_chars = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_chars = len(sentence)\n",
        "            if current_chars + sentence_chars > max_chars:\n",
        "                chunks.append(current_chunk)\n",
        "                current_chunk = sentence\n",
        "                current_chars = sentence_chars\n",
        "            else:\n",
        "                current_chunk += sentence + \"。\"\n",
        "                current_chars += sentence_chars\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(current_chunk)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    dat = df.copy()\n",
        "\n",
        "    results_rand = []\n",
        "\n",
        "    for index, row in dat.iterrows():\n",
        "        text = row['MDA']\n",
        "        ticker = row['ticker']\n",
        "        name = row['name']\n",
        "        closing_date = row['決算日']\n",
        "        reporting_date = row['報告日']\n",
        "        year = row['year']\n",
        "        month = row['month']\n",
        "        industry_33 = row['industry_33']\n",
        "        MDA_文字数 = row['MDA_文字数']\n",
        "\n",
        "        print(index, name, ticker)\n",
        "        text_chunks = split_text(text)\n",
        "\n",
        "        all_responses = []\n",
        "        all_input_tokens = []\n",
        "        all_output_tokens = []\n",
        "        chunk_texts = []\n",
        "        for chunk in text_chunks:\n",
        "            time.sleep(20)\n",
        "\n",
        "            retry_attempts = 0\n",
        "            max_retries = 5\n",
        "            while retry_attempts < max_retries:\n",
        "                try:\n",
        "                    response_rand = client.messages.create(\n",
        "                        model=api_model,\n",
        "                        system=instruction,\n",
        "                        messages=[\n",
        "                            {\"role\": \"user\", \"content\": chunk}\n",
        "                        ],\n",
        "                        temperature=0.3,\n",
        "                        max_tokens=500,\n",
        "                        top_p=0.5\n",
        "                    )\n",
        "                    response_text_rand = response_rand.content[0].text\n",
        "                    all_responses.append(response_text_rand)\n",
        "                    chunk_texts.append(response_text_rand)\n",
        "                    all_input_tokens.append(response_rand.usage.input_tokens)\n",
        "                    all_output_tokens.append(response_rand.usage.output_tokens)\n",
        "                    break\n",
        "\n",
        "                except Exception as e:\n",
        "                    retry_attempts += 1\n",
        "                    wait_time = 30\n",
        "                    print(f\"Rate limit exceeded: {e}. Retrying in {wait_time} seconds...\")\n",
        "                    time.sleep(wait_time)\n",
        "                    continue\n",
        "\n",
        "        posi_scores = []\n",
        "        nega_scores = []\n",
        "        posi_pattern = re.compile(r'positive_score\\s*[:：]\\s*([0-9]{1,3}(\\.[0-9]+)?)')\n",
        "        nega_pattern = re.compile(r'negative_score\\s*[:：]\\s*([0-9]{1,3}(\\.[0-9]+)?)')\n",
        "        for response in all_responses:\n",
        "            posi_match = posi_pattern.search(response)\n",
        "            nega_match = nega_pattern.search(response)\n",
        "            if posi_match:\n",
        "                posi_scores.append(float(posi_match.group(1)))\n",
        "            if nega_match:\n",
        "                nega_scores.append(float(nega_match.group(1)))\n",
        "\n",
        "        posi_score_avg = sum(posi_scores) / len(posi_scores) if posi_scores else None\n",
        "        nega_score_avg = sum(nega_scores) / len(nega_scores) if nega_scores else None\n",
        "\n",
        "        result_rand = {\n",
        "            \"ticker\": ticker,\n",
        "            \"name\": name,\n",
        "            \"報告日\": reporting_date,\n",
        "            \"決算日\": closing_date,\n",
        "            \"year\": year,\n",
        "            \"month\": month,\n",
        "            \"claude_positive\": posi_score_avg,\n",
        "            \"claude_negative\": nega_score_avg,\n",
        "            \"input_tokens\": all_input_tokens,\n",
        "            \"output_tokens\": all_output_tokens,\n",
        "            \"industry_33\": industry_33,\n",
        "            \"MDA_文字数\": MDA_文字数\n",
        "        }\n",
        "        if len(text_chunks) > 1:\n",
        "            result_rand[\"chunk_responses\"] = chunk_texts\n",
        "\n",
        "        results_rand.append(result_rand)\n",
        "\n",
        "    results_dat = pd.DataFrame(results_rand)\n",
        "\n",
        "    max_segments = max(results_dat['input_tokens'].apply(len))\n",
        "\n",
        "    for i in range(max_segments):\n",
        "        results_dat[f'input_tokens_{i+1}'] = results_dat['input_tokens'].apply(lambda x: x[i] if i < len(x) else None)\n",
        "        results_dat[f'output_tokens_{i+1}'] = results_dat['output_tokens'].apply(lambda x: x[i] if i < len(x) else None)\n",
        "\n",
        "    results_dat.drop(columns=['input_tokens', 'output_tokens'], inplace=True)\n",
        "    return results_dat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oh1l60dS5fix"
      },
      "outputs": [],
      "source": [
        "csv_file_path = './MDA_DataSet_2014_2022_TSE1.csv'\n",
        "df = pd.read_csv(csv_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXUX2N2Iy5iw"
      },
      "outputs": [],
      "source": [
        "api_model = \"claude-3-haiku-20240307\"\n",
        "\n",
        "result_df = analyze_sentiment(subset_df, api_model)\n",
        "\n",
        "print(result_df)\n",
        "\n",
        "result_df.to_csv('./MDA_DataSet_2014_2022_TSE1_Claude.csv', index=False, header=False, mode='a')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}