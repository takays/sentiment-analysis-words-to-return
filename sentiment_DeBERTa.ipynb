{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### DeBERTa sentiment analysis"
      ],
      "metadata": {
        "id": "pQIgcy8Dlq7m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAfs6WJxA30u",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IUq0EQUBlPb",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Kenkyu/Finance/2024/data/'\n",
        "%cd $path\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets pandas torch"
      ],
      "metadata": {
        "id": "gdDRmhz1P2md",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model_name = \"./DeBERTa/best_model_f1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "YRKXiY9p-Nyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_prefix(text):\n",
        "    pattern = r'.【.*?】'\n",
        "    result = re.sub(pattern, '', text)\n",
        "    return result"
      ],
      "metadata": {
        "id": "bmvdo9Pc-4t0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XNDFFmlBV2Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_chunk(chunk, text_column_index):\n",
        "    texts = chunk.iloc[:, text_column_index].tolist()\n",
        "    texts = [remove_prefix(text) for text in texts]\n",
        "    print(texts)\n",
        "\n",
        "    def preprocess_function(texts):\n",
        "        return tokenizer(texts, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "    inputs = preprocess_function(texts)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "        outputs = model(**inputs)\n",
        "        probs = F.softmax(outputs.logits, dim=-1)\n",
        "        positive_probs = probs[:, 1].cpu().numpy()\n",
        "        negative_probs = probs[:, 0].cpu().numpy()\n",
        "\n",
        "    select_columns = [0, 1, 2, 3, 4, 6, 7]\n",
        "    result_chunk = chunk.iloc[:, select_columns].copy()\n",
        "    result_chunk['positive_probability'] = positive_probs\n",
        "    result_chunk['negative_probability'] = negative_probs\n",
        "\n",
        "    return result_chunk"
      ],
      "metadata": {
        "id": "m0sIgd7xV6LM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "temp_dir = \"./temp_chunks\"\n",
        "os.makedirs(temp_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "pUaMw2nMgmpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_csv = './MDA_DataSet_2014_2022_TSE1.csv'\n",
        "output_csv = './MDA_DataSet_2014_2022_TSE1_deberta.csv'\n",
        "chunk_size = 50"
      ],
      "metadata": {
        "id": "HHSkJO7yX_D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_number = 0\n",
        "for chunk in pd.read_csv(input_csv, chunksize=chunk_size):\n",
        "    processed_chunk = process_chunk(chunk, text_column_index=5)\n",
        "    temp_chunk_file = os.path.join(temp_dir, f\"chunk_{chunk_number}.csv\")\n",
        "    processed_chunk.to_csv(temp_chunk_file, index=False)\n",
        "    chunk_number += 1\n",
        "\n",
        "header_written = False\n",
        "with open(output_csv, 'w', encoding='utf-8') as fout:\n",
        "    for i in range(chunk_number):\n",
        "        temp_chunk_file = os.path.join(temp_dir, f\"chunk_{i}.csv\")\n",
        "        with open(temp_chunk_file, 'r', encoding='utf-8') as fin:\n",
        "            if not header_written:\n",
        "                fout.write(fin.read())\n",
        "                header_written = True\n",
        "            else:\n",
        "                next(fin)\n",
        "                fout.write(fin.read())\n",
        "\n",
        "for i in range(chunk_number):\n",
        "    temp_chunk_file = os.path.join(temp_dir, f\"chunk_{i}.csv\")\n",
        "    os.remove(temp_chunk_file)\n",
        "os.rmdir(temp_dir)"
      ],
      "metadata": {
        "id": "fltLQWj4YPv7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}